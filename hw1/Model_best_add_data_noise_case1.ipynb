{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-OvNy23p4n7"
   },
   "source": [
    "## Noise adding for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Liq7POvhpMB-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wj8ncPohhFZw"
   },
   "source": [
    "For Data Preprocessing, first we deal with anomaly data, basically data with wrong or invalid format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3C1csUtng9c5"
   },
   "outputs": [],
   "source": [
    "def readdata(data):\n",
    "    \n",
    "\t# 把有些數字後面的奇怪符號刪除\n",
    "\tfor col in list(data.columns[2:]):\n",
    "\t\tdata[col] = data[col].astype(str).map(lambda x: x.rstrip('x*#A'))\n",
    "\tdata = data.values\n",
    "\t\n",
    "\t# 刪除欄位名稱及日期\n",
    "\tdata = np.delete(data, [0,1], 1)\n",
    "\t\n",
    "\t# 特殊值補0\n",
    "\tdata[ data == 'NR'] = 0\n",
    "\tdata[ data == ''] = 0\n",
    "\tdata[ data == 'nan'] = 0\n",
    "\tdata = data.astype(np.float)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duSP2H-1iIUg"
   },
   "source": [
    "We flatten our data to be in such format (col: one hour/ per col, row: one feature/ per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMIcdKlghxbQ"
   },
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "\tN = data.shape[0] // 18\n",
    "\n",
    "\ttemp = data[:18, :]\n",
    "    \n",
    "    # Shape 會變成 (x, 18) x = 取多少hours\n",
    "\tfor i in range(1, N):\n",
    "\t\ttemp = np.hstack((temp, data[i*18: i*18+18, :]))\n",
    "\treturn temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkG3UBm5jCbH"
   },
   "source": [
    "Since some data points (PM2.5) have anomaly values, which strongly effect our training result, we decide to abandon them. In our case, we define PM2.5 < 2 or > 100 as anomaly data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1i1dzcQjY3w"
   },
   "outputs": [],
   "source": [
    "def valid(x, y):\n",
    "\tif y <= 2 or y > 100:\n",
    "\t\treturn False\n",
    "\tfor i in range(9):\n",
    "\t\tif x[9,i] <= 2 or x[9,i] > 100:\n",
    "\t\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "def parse2train(data):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\t\n",
    "\t# 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "\ttotal_length = data.shape[1] - 9\n",
    "\tfor i in range(total_length):\n",
    "\t\tx_tmp = data[:,i:i+9]\n",
    "\t\ty_tmp = data[9,i+9]\n",
    "\t\tif valid(x_tmp, y_tmp):\n",
    "\t\t\tx.append(x_tmp.reshape(-1,))\n",
    "\t\t\ty.append(y_tmp)\n",
    "\t# x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "\tx = np.array(x)\n",
    "\ty = np.array(y)\n",
    "\treturn x,y\n",
    "\n",
    "def parse2test(data):\n",
    "\tx = []\n",
    "\t\n",
    "\ttotal_length = data.shape[1] // 9\n",
    "\tfor i in range(total_length):\n",
    "\t\tx_tmp = data[:,i*9:i*9+9]\n",
    "\t\tx.append(x_tmp.reshape(-1,))\n",
    "\t# x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "\tx = np.array(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjQZ_3VekIO4"
   },
   "source": [
    "This is our gradient descent algorithm. **Adam** was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcTr91tAkGgP"
   },
   "outputs": [],
   "source": [
    "def minibatch(x, y):\n",
    "    # 打亂data順序\n",
    "    index = np.arange(x.shape[0])\n",
    "    np.random.shuffle(index)\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    # 訓練參數以及初始化\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    lam = 0.001\n",
    "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n",
    "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n",
    "    w = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
    "    bias = 0.1\n",
    "    m_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    v_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    m_t_b = 0.0\n",
    "    v_t_b = 0.0\n",
    "    t = 0\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    for num in range(1000):\n",
    "        for b in range(int(x.shape[0]/batch_size)):\n",
    "            t+=1\n",
    "            x_batch = x[b*batch_size:(b+1)*batch_size]\n",
    "            y_batch = y[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
    "            loss = y_batch - np.dot(x_batch,w) - bias\n",
    "            \n",
    "            # 計算gradient\n",
    "            g_t = np.dot(x_batch.transpose(),loss) * (-2) +  2 * lam * np.sum(w)\n",
    "            g_t_b = loss.sum(axis=0) * (2)\n",
    "            m_t = beta_1*m_t + (1-beta_1)*g_t \n",
    "            v_t = beta_2*v_t + (1-beta_2)*np.multiply(g_t, g_t)\n",
    "            m_cap = m_t/(1-(beta_1**t))\n",
    "            v_cap = v_t/(1-(beta_2**t))\n",
    "            m_t_b = 0.9*m_t_b + (1-0.9)*g_t_b\n",
    "            v_t_b = 0.99*v_t_b + (1-0.99)*(g_t_b*g_t_b) \n",
    "            m_cap_b = m_t_b/(1-(0.9**t))\n",
    "            v_cap_b = v_t_b/(1-(0.99**t))\n",
    "            w_0 = np.copy(w)\n",
    "            \n",
    "            # 更新weight, bias\n",
    "            w -= ((lr*m_cap)/(np.sqrt(v_cap)+epsilon)).reshape(-1, 1)\n",
    "            bias -= (lr*m_cap_b)/(math.sqrt(v_cap_b)+epsilon)\n",
    "            \n",
    "\n",
    "    return w, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_once(path):\n",
    "    year1_pd = pd.read_csv(path)\n",
    "    year1 = readdata(year1_pd)\n",
    "    train_data = extract(year1)\n",
    "    return parse2train(train_data)\n",
    "    \n",
    "\n",
    "def read_once2(path1, path2):\n",
    "    # read y1\n",
    "    year1_pd = pd.read_csv(path1)\n",
    "    year1 = readdata(year1_pd)\n",
    "    train_data = extract(year1)\n",
    "    train_x, train_y = parse2train(train_data)\n",
    "    # Read y2\n",
    "    year2_pd = pd.read_csv(path2)    \n",
    "    year2 = readdata(year2_pd)\n",
    "    train_data2 = extract(year2)\n",
    "    train_x2, train_y2 = parse2train(train_data2)\n",
    "    \n",
    "    # concate\n",
    "    train_x = np.vstack((train_x,train_x2))\n",
    "    train_y = np.concatenate((train_y,train_y2))\n",
    "    return train_x, train_y\n",
    "# **Combine them together!**\n",
    "\n",
    "def read_test(path3):\n",
    "    testing_pd = pd.read_csv(path3)\n",
    "    testing = readdata(testing_pd)\n",
    "    testing_data = extract(testing)\n",
    "    return parse2test(testing_data)\n",
    "\n",
    "def read_item_list(path):\n",
    "    from collections import OrderedDict\n",
    "    year1_pd = pd.read_csv(path)\n",
    "    return list(OrderedDict.fromkeys(year1_pd.iloc[::,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKQ8D7FSnJiZ"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "path = './data/year2-data.csv'\n",
    "item_list = read_item_list(path)\n",
    "\n",
    "year_pd = pd.read_csv(path)\n",
    "year = readdata(year_pd)\n",
    "train_data = extract(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding noise in data as regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1:抽全部9小時內的污染源feature當作一次項(加bias)\n",
    "Adding noise before parsing.\n",
    "\n",
    "Noise is in raw data not in features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong code in noise level but better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38.43047691861617,\n",
       " 0.027521032583765977,\n",
       " 0.059790400636715244,\n",
       " 0.01164623556118513,\n",
       " 36.49385222355142,\n",
       " 102.65542733666418,\n",
       " 202.25939268103252,\n",
       " 369.1745158769626,\n",
       " 590.4693430078604,\n",
       " 7717.303667550197,\n",
       " 4.2946946263630865,\n",
       " 197.6025015319739,\n",
       " 5.525599986317008,\n",
       " 0.05338167208773795,\n",
       " 6141.835352129335,\n",
       " 6413.486185131539,\n",
       " 1.0198066240017931,\n",
       " 0.8722428033975521]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.var(train_data[i,::]) for i in range(len(train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5726, 162)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add noise here\n",
    "noise_level = 0.1\n",
    "train_data.shape\n",
    "for i in range(len(train_data)):\n",
    "# wrong code but better result\n",
    "#     train_data[i] = train_data[i] + np.random.normal(0, np.var(train_data[0,::])*noise_level, len(train_data[i]))\n",
    "# right one\n",
    "    if i != 9:\n",
    "        train_data[i] = train_data[i] + np.random.normal(0, (np.var(train_data[i,::])*noise_level)**(1/2), len(train_data[i]))\n",
    "    else:\n",
    "        train_data[i] = train_data[i] + np.random.normal(0, np.var(train_data[0,::])*noise_level, len(train_data[i]))\n",
    "train_x, train_y = parse2train(train_data)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.43839518200506,\n",
       " 0.030007787230198476,\n",
       " 0.06566074305586862,\n",
       " 0.012849231038008033,\n",
       " 40.05686951719266,\n",
       " 112.25147232075302,\n",
       " 222.51666993065038,\n",
       " 406.5096374934065,\n",
       " 647.8534882543198,\n",
       " 7742.752825652213,\n",
       " 4.732297075460375,\n",
       " 218.97288099748417,\n",
       " 6.110712135370229,\n",
       " 0.05839445618139722,\n",
       " 6699.993985166474,\n",
       " 7021.502590797796,\n",
       " 1.1160657488873078,\n",
       " 0.9578207709381813]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.var(train_data[i,::]) for i in range(len(train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "w, bias = minibatch(train_x, train_y)\n",
    "\n",
    "np.save('best_w.npy', w)\n",
    "np.save('best_bias.npy',bias)\n",
    "\n",
    "# read testing and pred\n",
    "# test_path = './data/testing_data.csv'\n",
    "# test_x = read_test(test_path)\n",
    "\n",
    "# pred_y = test_x @ w + bias\n",
    "# id_list = [ \"id_\"+str(i) for i in range(len(test_x))]\n",
    "# output_pd = pd.DataFrame(zip(id_list, pred_y.ravel()), columns=['id', 'value'])\n",
    "\n",
    "# output_name = './output/y2_data_noise0.1_dataSel_noiseReg.csv'\n",
    "# output_pd.to_csv(output_name, index=False)\n",
    "\n",
    "# train_y_pred = train_x @ w + bias\n",
    "# err = np.mean((train_y_pred.ravel() - train_y.ravel())**2)\n",
    "# err"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not yet finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column selection by cross validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def item_hour2col_sel(it_hour):\n",
    "    col_sel = np.ones(9*18)\n",
    "    for i in range(len(it_hour)):\n",
    "        hour = int(9 - it_hour[i])\n",
    "        if hour != 0:\n",
    "            col_sel[ i*9 : i*9 + hour] = 0\n",
    "    return col_sel\n",
    "\n",
    "def MSE(a,b):\n",
    "    return np.mean((a.ravel() - b.ravel())**2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = Ridge(tol = 10e-6)\n",
    "\n",
    "\n",
    "item_hour_init = np.ones(18)*2\n",
    "\n",
    "item_err_list = []\n",
    "item_cv_err_list = []\n",
    "for j in range(len(item_hour)):\n",
    "    tmp_item_hour = item_hour_init.copy()\n",
    "    tmp_item_hour[j] += 2\n",
    "#     cross validation\n",
    "\n",
    "    col = item_hour2col_sel(tmp_item_hour).astype(bool)\n",
    "    train_x_tmp = train_x[::,col]\n",
    "    train_y_tmp = train_y.copy()\n",
    "    err_list = []\n",
    "    for i in range(5):\n",
    "        # train_t\n",
    "        train_x_cv, test_x_cv, train_y_cv, test_y_cv = train_test_split(\n",
    "            train_x_tmp, train_y_tmp, test_size=0.1, random_state=i+j)\n",
    "        w_cv, bias_cv = minibatch(train_x_cv, train_y_cv)\n",
    "        cv_pred = Ridge.fit(train_x_cv, train_y_cv).predict(test_x_cv)\n",
    "#         cv_pred = test_x_cv @ w_cv + bias_cv\n",
    "#         err = MSE( test_y_cv, test_x_cv @ w_cv + bias_cv )\n",
    "        err_list.append(err)\n",
    "#     size of it_cv_err_list = 18\n",
    "    item_cv_err_list.append(np.mean(err_list))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "item_cv_err_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from scipy import stats\n",
    "# stats.describe(train_data[0,::])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLhw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
