{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-OvNy23p4n7"
   },
   "source": [
    "## ML HW1 手把手教學"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Liq7POvhpMB-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Only Needed on Google Colab\n",
    "#from google.colab import files\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wj8ncPohhFZw"
   },
   "source": [
    "For Data Preprocessing, first we deal with anomaly data, basically data with wrong or invalid format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3C1csUtng9c5"
   },
   "outputs": [],
   "source": [
    "def readdata(data):\n",
    "    \n",
    "\t# 把有些數字後面的奇怪符號刪除\n",
    "\tfor col in list(data.columns[2:]):\n",
    "\t\tdata[col] = data[col].astype(str).map(lambda x: x.rstrip('x*#A'))\n",
    "\tdata = data.values\n",
    "\t\n",
    "\t# 刪除欄位名稱及日期\n",
    "\tdata = np.delete(data, [0,1], 1)\n",
    "\t\n",
    "\t# 特殊值補0\n",
    "\tdata[ data == 'NR'] = 0\n",
    "\tdata[ data == ''] = 0\n",
    "\tdata[ data == 'nan'] = 0\n",
    "\tdata = data.astype(np.float)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duSP2H-1iIUg"
   },
   "source": [
    "We flatten our data to be in such format (col: one hour/ per col, row: one feature/ per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMIcdKlghxbQ"
   },
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "\tN = data.shape[0] // 18\n",
    "\n",
    "\ttemp = data[:18, :]\n",
    "    \n",
    "    # Shape 會變成 (x, 18) x = 取多少hours\n",
    "\tfor i in range(1, N):\n",
    "\t\ttemp = np.hstack((temp, data[i*18: i*18+18, :]))\n",
    "\treturn temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkG3UBm5jCbH"
   },
   "source": [
    "Since some data points (PM2.5) have anomaly values, which strongly effect our training result, we decide to abandon them. In our case, we define PM2.5 < 2 or > 100 as anomaly data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1i1dzcQjY3w"
   },
   "outputs": [],
   "source": [
    "def valid(x, y):\n",
    "\tif y <= 2 or y > 100:\n",
    "\t\treturn False\n",
    "\tfor i in range(9):\n",
    "\t\tif x[9,i] <= 2 or x[9,i] > 100:\n",
    "\t\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "def parse2train(data):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\t\n",
    "\t# 用前面9筆資料預測下一筆PM2.5 所以需要-9\n",
    "\ttotal_length = data.shape[1] - 9\n",
    "\tfor i in range(total_length):\n",
    "\t\tx_tmp = data[:,i:i+9]\n",
    "\t\ty_tmp = data[9,i+9]\n",
    "\t\tif valid(x_tmp, y_tmp):\n",
    "\t\t\tx.append(x_tmp.reshape(-1,))\n",
    "\t\t\ty.append(y_tmp)\n",
    "\t# x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "\tx = np.array(x)\n",
    "\ty = np.array(y)\n",
    "\treturn x,y\n",
    "\n",
    "def parse2test(data):\n",
    "\tx = []\n",
    "\t\n",
    "\ttotal_length = data.shape[1] // 9\n",
    "\tfor i in range(total_length):\n",
    "\t\tx_tmp = data[:,i*9:i*9+9]\n",
    "\t\tx.append(x_tmp.reshape(-1,))\n",
    "\t# x 會是一個(n, 18, 9)的陣列， y 則是(n, 1) \n",
    "\tx = np.array(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjQZ_3VekIO4"
   },
   "source": [
    "This is our gradient descent algorithm. **Adam** was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcTr91tAkGgP"
   },
   "outputs": [],
   "source": [
    "def minibatch(x, y):\n",
    "    # 打亂data順序\n",
    "    index = np.arange(x.shape[0])\n",
    "    np.random.shuffle(index)\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    # 訓練參數以及初始化\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    lam = 0.001\n",
    "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n",
    "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n",
    "    w = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
    "    bias = 0.1\n",
    "    m_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    v_t = np.full(x[0].shape, 0).reshape(-1, 1)\n",
    "    m_t_b = 0.0\n",
    "    v_t_b = 0.0\n",
    "    t = 0\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    for num in range(1000):\n",
    "        for b in range(int(x.shape[0]/batch_size)):\n",
    "            t+=1\n",
    "            x_batch = x[b*batch_size:(b+1)*batch_size]\n",
    "            y_batch = y[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
    "            loss = y_batch - np.dot(x_batch,w) - bias\n",
    "            \n",
    "            # 計算gradient\n",
    "            g_t = np.dot(x_batch.transpose(),loss) * (-2) +  2 * lam * np.sum(w)\n",
    "            g_t_b = loss.sum(axis=0) * (2)\n",
    "            m_t = beta_1*m_t + (1-beta_1)*g_t \n",
    "            v_t = beta_2*v_t + (1-beta_2)*np.multiply(g_t, g_t)\n",
    "            m_cap = m_t/(1-(beta_1**t))\n",
    "            v_cap = v_t/(1-(beta_2**t))\n",
    "            m_t_b = 0.9*m_t_b + (1-0.9)*g_t_b\n",
    "            v_t_b = 0.99*v_t_b + (1-0.99)*(g_t_b*g_t_b) \n",
    "            m_cap_b = m_t_b/(1-(0.9**t))\n",
    "            v_cap_b = v_t_b/(1-(0.99**t))\n",
    "            w_0 = np.copy(w)\n",
    "            \n",
    "            # 更新weight, bias\n",
    "            w -= ((lr*m_cap)/(np.sqrt(v_cap)+epsilon)).reshape(-1, 1)\n",
    "            bias -= (lr*m_cap_b)/(math.sqrt(v_cap_b)+epsilon)\n",
    "            \n",
    "\n",
    "    return w, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKQ8D7FSnJiZ"
   },
   "source": [
    "**Combine them together!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wy0tfd5LnJCx",
    "outputId": "151109e0-b92e-465a-9955-6897b803cb4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 162) (6938,)\n",
      "(14038, 162) (14038,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 同學這邊要自己吃csv files\n",
    "    #uploaded = files.upload()\n",
    "    \n",
    "    # Read y1\n",
    "    year1_pd = pd.read_csv('./data/year1-data.csv')\n",
    "    year1 = readdata(year1_pd)\n",
    "    train_data = extract(year1)\n",
    "    train_x, train_y = parse2train(train_data)\n",
    "    print(train_x.shape, train_y.shape)\n",
    "\n",
    "    # Read y2\n",
    "    year2_pd = pd.read_csv('./data/year2-data.csv')    \n",
    "    year2 = readdata(year2_pd)\n",
    "    train_data2 = extract(year2)\n",
    "    train_x2, train_y2 = parse2train(train_data2)\n",
    "    # concate\n",
    "    train_x = np.vstack((train_x,train_x2))\n",
    "    train_y = np.concatenate((train_y,train_y2))\n",
    "    print(train_x.shape, train_y.shape)\n",
    "\n",
    "#     w, bias = minibatch(train_x, train_y)\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # Fit regression model\n",
    "#     params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "#               'learning_rate': 0.01, 'loss': 'ls'}\n",
    "#     clf = GradientBoostingRegressor(**params)\n",
    "    clf = GradientBoostingRegressor()\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    train_err = mean_squared_error(train_y, clf.predict(train_x))\n",
    "#     mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "#     print(\"MSE: %.4f\" % mse)\n",
    "    \n",
    "    # read testing\n",
    "    testing_pd = pd.read_csv('./data/testing_data.csv')\n",
    "    testing = readdata(testing_pd)\n",
    "    testing_data = extract(testing)\n",
    "    testing_x = parse2test(testing_data)\n",
    "\n",
    "    #     pred_y = testing_x @ w\n",
    "    pred_y = clf.predict(testing_x)\n",
    "    output = pd.read_csv('./data/sample_submission.csv')\n",
    "    output['value'] = pred_y\n",
    "    output.to_csv('./data/output/gb_default_y1y2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.236474431215752, 14.018810824603339)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "train_err = mean_squared_error(train_y, clf.predict(train_x))\n",
    "train_err, train_err2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y1y2\n",
    "default\n",
    "15.236474431215752\n",
    "\n",
    "y2\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "14.018810824603339\n",
    "5.68339\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLhw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
