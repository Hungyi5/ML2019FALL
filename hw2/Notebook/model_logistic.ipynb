{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeZtEkZdNMGg"
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "**To Do**\n",
    "\n",
    "    1. regularization\n",
    "    \n",
    "    2. kernel (feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMOeb2gcNWdB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLvVaOeANqmH"
   },
   "source": [
    "We only use one-hot-encoding feature here\n",
    "\n",
    "[Shell script usage](https://hackmd.io/@NeYbO-fDS5-UW6DQTmpVBA/HJIiFdZur?fbclid=IwAR0zGWEENLKgk3pmyng7CzUloZsHD0DDtYsNzumXzI2DIPZ9aoaluq-5WDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4uvD-jLNrWM"
   },
   "outputs": [],
   "source": [
    "def load_data(path_x_train, path_y_train, path_test):\n",
    "\\    x_train = pd.read_csv(path_x_train)\n",
    "    x_test = pd.read_csv(path_test)\n",
    "\n",
    "    x_train = x_train.values\n",
    "    x_test = x_test.values\n",
    "\n",
    "    y_train = pd.read_csv(path_y_train, header = None)\n",
    "    y_train = y_train.values\n",
    "    y_train = y_train.reshape(-1)\n",
    "\n",
    "    return x_train, y_train, x_test\n",
    "\n",
    "# Use np.clip to prevent overflow\n",
    "def sigmoid(z):\n",
    "    res = 1 / (1.0 + np.exp(-z))\n",
    "    return np.clip(res, 1e-6, 1-1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JauhKOL8PoaP"
   },
   "source": [
    "Feature normalize, only on continues variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvUJFu5gP1ac"
   },
   "outputs": [],
   "source": [
    "def normalize(x_train, x_test):\n",
    "    \n",
    "    x_all = np.concatenate((x_train, x_test), axis = 0)\n",
    "    mean = np.mean(x_all, axis = 0)\n",
    "    std = np.std(x_all, axis = 0)\n",
    "\n",
    "    index = [0, 1, 3, 4, 5]\n",
    "    mean_vec = np.zeros(x_all.shape[1])\n",
    "    std_vec = np.ones(x_all.shape[1])\n",
    "    mean_vec[index] = mean[index]\n",
    "    std_vec[index] = std[index]\n",
    "\n",
    "    x_all_nor = (x_all - mean_vec) / std_vec\n",
    "\n",
    "    x_train_nor = x_all_nor[0:x_train.shape[0]]\n",
    "    x_test_nor = x_all_nor[x_train.shape[0]:]\n",
    "\n",
    "    return x_train_nor, x_test_nor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txYrOUqsQKCD"
   },
   "source": [
    "Gradient descent using adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA3DI4BzQN6l"
   },
   "outputs": [],
   "source": [
    "def train(x_train, y_train):\n",
    "    b = 0.0\n",
    "    w = np.zeros(x_train.shape[1])\n",
    "    lr = 0.05\n",
    "    epoch = 1000\n",
    "    b_lr = 0\n",
    "    w_lr = np.ones(x_train.shape[1])\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        z = np.dot(x_train, w) + b\n",
    "        pred = sigmoid(z)\n",
    "        loss = y_train - pred\n",
    "\n",
    "        b_grad = -1*np.sum(loss)\n",
    "        w_grad = -1*np.dot(loss, x_train)\n",
    "\n",
    "        b_lr += b_grad**2\n",
    "        w_lr += w_grad**2\n",
    "\n",
    "\n",
    "        b = b-lr/np.sqrt(b_lr)*b_grad\n",
    "        w = w-lr/np.sqrt(w_lr)*w_grad\n",
    "\n",
    "        if(e+1)%500 == 0:\n",
    "            loss = -1*np.mean(y_train*np.log(pred+1e-100) + (1-y_train)*np.log(1-pred+1e-100))\n",
    "            print('epoch:{}\\nloss:{}\\n'.format(e+1,loss))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClofrDqhQVU8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45tuF_lJQVos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:500\n",
      "loss:0.32267305628139425\n",
      "\n",
      "epoch:1000\n",
      "loss:0.3185692738127604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    if sys.argv[5] == '--test':\n",
    "        x_train, y_train, x_test = load_data(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "        wb = np.load('log_wb.npy')\n",
    "        w = wb[0:-1]\n",
    "        b = wb[-1]\n",
    "        y_score = np.dot(x_test, w) + b\n",
    "        pred_y = sigmoid(y_score)\n",
    "        pred_y = (pred_y > 0.5).astype(int)\n",
    "\n",
    "        output = zip(list(range(1,len(x_test)+1)), pred_y.ravel())\n",
    "        output = pd.DataFrame(output, columns = ['id','label'])\n",
    "        output_name = sys.argv[4]\n",
    "        output.to_csv(output_name, index=False)\n",
    "        \n",
    "    elif sys.argv[5] == '--train':\n",
    "        path_x_train= './data/X_train'\n",
    "        path_y_train='./data/Y_train'\n",
    "        path_test='./data/X_test'\n",
    "        x_train, y_train, x_test = load_data(path_x_train, path_y_train, path_test)\n",
    "\n",
    "        x_train, x_test = normalize(x_train, x_test)\n",
    "        # perform worse\n",
    "#         for idx, val in enumerate(x_train.shape):\n",
    "#             x_train[idx] =  val + np.random.normal(0, 0.1**(1/2) ,x_train.shape[1])\n",
    "        w, b = train(x_train, y_train)\n",
    "\n",
    "        y_score = np.dot(x_test, w) + b\n",
    "        pred_y = sigmoid(y_score)\n",
    "        pred_y = (pred_y > 0.5).astype(int)\n",
    "\n",
    "        wb = np.insert(w,len(w),b)\n",
    "        np.save('log_wb.npy',wb)\n",
    "\n",
    "#         output = zip(list(range(1,len(x_test)+1)), pred_y.ravel())\n",
    "#         output = pd.DataFrame(output, columns = ['id','label'])\n",
    "#         output_name = './output/log_tutorial_unnormalize.csv'\n",
    "#         output.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3aDCdTxXo-B"
   },
   "source": [
    "### Tip for math problem\n",
    "[p1](https://people.eecs.berkeley.edu/~jrs/189/exam/mids14.pdf)  \n",
    "[p2&3](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf)  \n",
    "[p3](https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLhw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:python3.6] *",
   "language": "python",
   "name": "conda-env-python3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
